{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"datasetVersion","sourceId":3263022,"datasetId":1976854,"databundleVersionId":3313436},{"sourceType":"datasetVersion","sourceId":2702383,"datasetId":1646010,"databundleVersionId":2747084},{"sourceType":"datasetVersion","sourceId":7380165,"datasetId":4288718,"databundleVersionId":7471005}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom torch.utils.data import Dataset, Subset, DataLoader\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import SubsetRandomSampler\nfrom sklearn.preprocessing import LabelEncoder\n#import torch_xla.distributed.parallel_loader as pl\n#import torch_xla.core.xla_model as xm\n\n#device = xm.xla_device()\n\nclass SignLanguageDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.data = datasets.ImageFolder(root_dir, transform=transform)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n    \nclass NpyDataset(Dataset):\n    def __init__(self, npy_file_path):\n        # Load the dataset\n        dataset_inputs = np.load(npy_file_path + '/X.npy', allow_pickle=True)\n        dataset_labels = np.load(npy_file_path + '/Y.npy', allow_pickle=True)\n        # Assuming your dataset has 'inputs' and 'labels' keys\n         # Convert labels to numerical representation using LabelEncoder\n        label_encoder = LabelEncoder()\n        dataset_labels_int = label_encoder.fit_transform(np.ravel(dataset_labels))\n        self.inputs = torch.tensor(dataset_inputs, dtype=torch.float32)\n        self.labels = torch.tensor(dataset_labels_int, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.inputs)\n\n    def __getitem__(self, index):\n        input_sample = self.inputs[index]\n        label = self.labels[index]\n        \n        #input_sample = torch.tensor(input_sample).permute(2, 0, 1)\n        return input_sample, label\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-11T01:37:04.945578Z","iopub.execute_input":"2024-01-11T01:37:04.946004Z","iopub.status.idle":"2024-01-11T01:37:04.961267Z","shell.execute_reply.started":"2024-01-11T01:37:04.945969Z","shell.execute_reply":"2024-01-11T01:37:04.960354Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ndataset = NpyDataset('/kaggle/input/27-class-sign-language-dataset')\n\ndataset.inputs = torch.tensor(dataset.inputs).permute(0, 3, 1, 2)\n\n#dataset = SignLanguageDataset(root_dir='/kaggle/input/aslamerican-sign-language-aplhabet-dataset', transform=transform)\n\n#print(len(dataset))\n#reduce_ratio = 0.1\n#smallset_indices = torch.randperm(len(dataset))[:int(len(dataset)* reduce_ratio)]\n#dataset = Subset(dataset, smallset_indices)\n\nprint(len(dataset))\n\ntrain_size = int(0.8 * len(dataset))\nval_size = int(0.1 * len(dataset))\ntest_size = len(dataset) - train_size - val_size\n\nprint(\"Length:\", train_size, val_size, test_size)\n\ntrain_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n    dataset, [train_size, val_size, test_size]\n)\n\n#device = xm.xla_device()\n#train_loader = pl.MpDeviceLoader(DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4), device)\n#val_loader = pl.MpDeviceLoader(DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4), device)\n#test_loader = pl.MpDeviceLoader(DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4), device)\n\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, pin_memory=True, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, pin_memory=True, num_workers=4)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, pin_memory=True, num_workers=4)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-11T01:37:04.963286Z","iopub.execute_input":"2024-01-11T01:37:04.963622Z","iopub.status.idle":"2024-01-11T01:37:39.697380Z","shell.execute_reply.started":"2024-01-11T01:37:04.963579Z","shell.execute_reply":"2024-01-11T01:37:39.696147Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_42/863373535.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  dataset.inputs = torch.tensor(dataset.inputs).permute(0, 3, 1, 2)\n","output_type":"stream"},{"name":"stdout","text":"22801\nLength: 18240 2280 2281\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch.nn as nn\nfrom torchvision import models\n\nclass SignLanguageResNet(nn.Module):\n    def __init__(self, num_classes):\n        super(SignLanguageResNet, self).__init__()\n        resnet18 = models.resnet18(weights=True)\n        in_features = resnet18.fc.in_features\n        resnet18.fc = nn.Linear(in_features, num_classes)\n        self.resnet18 = resnet18\n\n    def forward(self, x):\n        return self.resnet18(x)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-11T01:37:39.698804Z","iopub.execute_input":"2024-01-11T01:37:39.699126Z","iopub.status.idle":"2024-01-11T01:37:39.706023Z","shell.execute_reply.started":"2024-01-11T01:37:39.699098Z","shell.execute_reply":"2024-01-11T01:37:39.705000Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"#model = SignLanguageResNet(num_classes=29)\n\nmodel = SignLanguageResNet(num_classes=27).cuda()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\ncriterion = nn.CrossEntropyLoss()\n#optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n#optimizer = xm.optimizer.Optimizer(optimizer)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-11T01:37:39.709116Z","iopub.execute_input":"2024-01-11T01:37:39.709561Z","iopub.status.idle":"2024-01-11T01:37:40.480905Z","shell.execute_reply.started":"2024-01-11T01:37:39.709524Z","shell.execute_reply":"2024-01-11T01:37:40.480009Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"}]},{"cell_type":"code","source":"num_epochs = 10\ncount = 0\n\nfor epoch in range(num_epochs):\n    # Training\n    model.train()\n    #para_loader = pl.ParallelLoader(train_loader, [device])\n    \n    for inputs, labels in train_loader:\n        optimizer.zero_grad()\n        outputs = model(inputs.cuda())\n        loss = criterion(outputs, labels.cuda())\n        loss.backward()\n        optimizer.step()\n        #xm.optimizer_step(optimizer, barrier=True)\n    \n    # Validation\n    model.eval()\n    val_loss = 0.0\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            outputs = model(inputs.cuda())\n            loss = criterion(outputs, labels.cuda())\n            val_loss += loss.item()\n\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels.cuda()).sum().item()\n\n    avg_val_loss = val_loss / len(val_loader)\n    val_accuracy = 100.0 * correct / total\n\n    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Validation Loss: {avg_val_loss}, Validation Accuracy: {val_accuracy}%')\n\ntorch.save(model.state_dict(), 'sign_language_resnet_model.pth')","metadata":{"execution":{"iopub.status.busy":"2024-01-11T01:37:40.482158Z","iopub.execute_input":"2024-01-11T01:37:40.482477Z","iopub.status.idle":"2024-01-11T01:40:05.499369Z","shell.execute_reply.started":"2024-01-11T01:37:40.482450Z","shell.execute_reply":"2024-01-11T01:40:05.497990Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"Epoch 1/10, Loss: 0.44647058844566345, Validation Loss: 0.3206897309670846, Validation Accuracy: 90.26315789473684%\nEpoch 2/10, Loss: 0.1408340036869049, Validation Loss: 0.15844487440254953, Validation Accuracy: 95.6140350877193%\nEpoch 3/10, Loss: 0.03273290768265724, Validation Loss: 0.10534250543504539, Validation Accuracy: 97.28070175438596%\nEpoch 4/10, Loss: 0.4026080071926117, Validation Loss: 0.5145644024014473, Validation Accuracy: 87.54385964912281%\nEpoch 5/10, Loss: 0.05722338706254959, Validation Loss: 0.09538260785241921, Validation Accuracy: 96.97368421052632%\nEpoch 6/10, Loss: 0.1284734606742859, Validation Loss: 0.11185922760826846, Validation Accuracy: 97.14912280701755%\nEpoch 7/10, Loss: 0.013536369428038597, Validation Loss: 0.07345504533602959, Validation Accuracy: 98.15789473684211%\nEpoch 8/10, Loss: 0.12999612092971802, Validation Loss: 0.1194199787827933, Validation Accuracy: 97.5%\nEpoch 9/10, Loss: 0.02975156344473362, Validation Loss: 0.0952602108526561, Validation Accuracy: 97.36842105263158%\nEpoch 10/10, Loss: 0.16061602532863617, Validation Loss: 0.08398633895719992, Validation Accuracy: 98.02631578947368%\n","output_type":"stream"}]},{"cell_type":"code","source":"# Testing Loop\nmodel.eval()\ntest_correct = 0\ntest_total = 0\n\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        outputs = model(inputs.cuda())\n        _, predicted = outputs.max(1)\n        test_total += labels.size(0)\n        test_correct += predicted.eq(labels.cuda()).sum().item()\n\ntest_accuracy = 100.0 * test_correct / test_total\nprint(f'Test Accuracy: {test_accuracy}%')","metadata":{"execution":{"iopub.status.busy":"2024-01-11T01:40:05.501645Z","iopub.execute_input":"2024-01-11T01:40:05.502577Z","iopub.status.idle":"2024-01-11T01:40:06.979298Z","shell.execute_reply.started":"2024-01-11T01:40:05.502526Z","shell.execute_reply":"2024-01-11T01:40:06.977804Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"Test Accuracy: 97.85181937746603%\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom PIL import Image\nimport torchvision.transforms as transforms\n\n# Assuming 'model' is your trained SignLanguageResNet model\n# Assuming the model is already on the GPU (model.cuda() was called)\n\n# Load the image\nimage_path = '/kaggle/input/imagem/test2.jpg'\nimage = Image.open(image_path)\n\n# Apply transformation\n\ninput_tensor = transform(image)\ninput_batch = input_tensor.unsqueeze(0)  # Add batch dimension\n\n# Move input to GPU\ninput_batch = input_batch.cuda()\n\n# Perform inference\n#model.eval()\nwith torch.no_grad():\n    output = model(input_batch)\n\n# Interpret the output (assuming it's a classification task)\npredicted_class = torch.argmax(output).item()\nprint(\"Predicted Class:\", predicted_class)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-11T01:46:36.054495Z","iopub.execute_input":"2024-01-11T01:46:36.055562Z","iopub.status.idle":"2024-01-11T01:46:36.081928Z","shell.execute_reply.started":"2024-01-11T01:46:36.055525Z","shell.execute_reply":"2024-01-11T01:46:36.080787Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"Predicted Class: 10\n","output_type":"stream"}]}]}